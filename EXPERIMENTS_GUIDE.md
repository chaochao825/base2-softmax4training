# Base-2 Softmax vs Standard Softmaxå®éªŒæŒ‡å—

## ğŸ“‹ å®éªŒæ¦‚è§ˆ

æœ¬é¡¹ç›®ç³»ç»Ÿæ€§åœ°å¯¹æ¯”äº†åœ¨BitNet 1.58-bité‡åŒ–åœºæ™¯ä¸‹ï¼Œ**Standard Softmax (base-e)** ä¸ **Base-2 Softmax** çš„æ€§èƒ½å·®å¼‚ã€‚

### æ ¸å¿ƒç ”ç©¶é—®é¢˜

1. **ç¨³å®šæ€§**: Base-2 Softmaxæ˜¯å¦èƒ½åœ¨è¶…ä½æ¯”ç‰¹é‡åŒ–ç¯å¢ƒä¸­æä¾›æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§ï¼Ÿ
2. **æ€§èƒ½**: ä¸¤ç§Softmaxå¯¹æœ€ç»ˆæ¨¡å‹å‡†ç¡®ç‡çš„å½±å“å¦‚ä½•ï¼Ÿ
3. **æ¢¯åº¦**: Base-2 Softmaxæ˜¯å¦äº§ç”Ÿæ›´å¹³æ»‘çš„æ¢¯åº¦ï¼Ÿ

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè®¾ç½®

```bash
# æ¿€æ´»condaç¯å¢ƒ
conda activate base-2-bitnet

# éªŒè¯GPUå¯ç”¨æ€§
nvidia-smi

# éªŒè¯ä¾èµ–
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
```

### è¿è¡Œå•ä¸ªå®éªŒ

#### å®éªŒ1: ResNet-18 on CIFAR-10

```bash
# Standard Softmax
python scripts/train_enhanced.py \
    --dataset CIFAR-10 \
    --model resnet18 \
    --batch_size 128 \
    --epochs 50 \
    --lr 1e-3 \
    --softmax standard \
    --scheduler cosine \
    --warmup_epochs 5 \
    --track_grads \
    --save_checkpoints

# Base-2 Softmax
python scripts/train_enhanced.py \
    --dataset CIFAR-10 \
    --model resnet18 \
    --batch_size 128 \
    --epochs 50 \
    --lr 1e-3 \
    --softmax base2 \
    --scheduler cosine \
    --warmup_epochs 5 \
    --track_grads \
    --save_checkpoints
```

#### å®éªŒ2: ViT-Small on CIFAR-10

```bash
# Standard Softmax
python scripts/train_enhanced.py \
    --dataset CIFAR-10 \
    --model vit-s \
    --batch_size 128 \
    --epochs 100 \
    --lr 5e-4 \
    --softmax standard \
    --scheduler cosine \
    --warmup_epochs 10 \
    --track_grads \
    --save_checkpoints

# Base-2 Softmax
python scripts/train_enhanced.py \
    --dataset CIFAR-10 \
    --model vit-s \
    --batch_size 128 \
    --epochs 100 \
    --lr 5e-4 \
    --softmax base2 \
    --scheduler cosine \
    --warmup_epochs 10 \
    --track_grads \
    --save_checkpoints
```

#### å®éªŒ3: ImageNet-100 (éœ€è¦ä¸‹è½½æ•°æ®)

```bash
# ä¸‹è½½ImageNet-100åˆ° /amax/storage/nfs/spco/data/imagenet/

# ViT-Base on ImageNet-100, Standard Softmax
python scripts/train_enhanced.py \
    --dataset ImageNet-100 \
    --data_path /amax/storage/nfs/spco/data \
    --model vit-b \
    --batch_size 64 \
    --epochs 150 \
    --lr 3e-4 \
    --softmax standard \
    --scheduler cosine \
    --warmup_epochs 15 \
    --track_grads \
    --save_checkpoints

# ViT-Base on ImageNet-100, Base-2 Softmax
python scripts/train_enhanced.py \
    --dataset ImageNet-100 \
    --data_path /amax/storage/nfs/spco/data \
    --model vit-b \
    --batch_size 64 \
    --epochs 150 \
    --lr 3e-4 \
    --softmax base2 \
    --scheduler cosine \
    --warmup_epochs 15 \
    --track_grads \
    --save_checkpoints
```

#### å®éªŒ4: LLM (TinyLlama on TinyStories)

```bash
# Standard Softmax
python scripts/train_llm.py \
    --model_name TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T \
    --dataset roneneldan/TinyStories \
    --softmax standard \
    --batch_size 4 \
    --epochs 1 \
    --num_train_samples 5000 \
    --max_length 512

# Base-2 Softmax
python scripts/train_llm.py \
    --model_name TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T \
    --dataset roneneldan/TinyStories \
    --softmax base2 \
    --batch_size 4 \
    --epochs 1 \
    --num_train_samples 5000 \
    --max_length 512
```

### æ‰¹é‡è¿è¡Œæ‰€æœ‰å®éªŒ

```bash
# è¿è¡Œæ‰€æœ‰å›¾åƒåˆ†ç±»å®éªŒ
bash scripts/run_quick_experiments.sh

# æŸ¥çœ‹è¿›åº¦
tail -f logs/full_experiments.log
```

---

## ğŸ“Š ç»“æœåˆ†æ

### ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š

```bash
python scripts/generate_report.py
```

è¿™å°†ç”Ÿæˆï¼š
- `results/comparison_report_accuracy_comparison.png` - å‡†ç¡®ç‡å¯¹æ¯”æ›²çº¿
- `results/comparison_report_loss_comparison.png` - æŸå¤±å¯¹æ¯”æ›²çº¿
- `results/comparison_report_gradient_norms.png` - æ¢¯åº¦èŒƒæ•°å¯¹æ¯”
- `results/comparison_report_final_comparison.png` - æœ€ç»ˆæ€§èƒ½æŸ±çŠ¶å›¾
- `results/summary.txt` - æ–‡æœ¬æ ¼å¼æ€»ç»“

### æŸ¥çœ‹å•ä¸ªå®éªŒç»“æœ

```bash
# æŸ¥çœ‹JSONç»“æœ
cat results/results_cifar10_resnet18_standard.json
cat results/results_cifar10_resnet18_base2.json

# æŸ¥çœ‹è®­ç»ƒæ›²çº¿
ls results/curves_*.png
```

---

## ğŸ”§ é«˜çº§é€‰é¡¹

### å¤šGPUè®­ç»ƒ

```bash
# ä½¿ç”¨torchrunè¿›è¡ŒDDPè®­ç»ƒ
torchrun --nproc_per_node=3 scripts/train_enhanced.py \
    --dataset CIFAR-10 \
    --model resnet18 \
    --batch_size 256 \
    --epochs 50 \
    --softmax base2 \
    --track_grads
```

### WandBé›†æˆ

```bash
# å¯ç”¨Weights & Biasesæ—¥å¿—
python scripts/train_enhanced.py \
    --dataset CIFAR-10 \
    --model resnet18 \
    --softmax base2 \
    --wandb
```

### æ¢å¤è®­ç»ƒ

```bash
# ä»checkpointæ¢å¤
python scripts/train_enhanced.py \
    --dataset CIFAR-10 \
    --model resnet18 \
    --softmax base2 \
    --resume results/best_resnet18_CIFAR-10_base2.pth
```

### è¶…å‚æ•°è°ƒä¼˜

```bash
# è°ƒæ•´æ¸©åº¦å‚æ•°
python scripts/train_enhanced.py \
    --dataset CIFAR-10 \
    --model resnet18 \
    --softmax base2 \
    --temperature 0.5  # å°è¯•ä¸åŒæ¸©åº¦

# è°ƒæ•´å­¦ä¹ ç‡å’Œè°ƒåº¦å™¨
python scripts/train_enhanced.py \
    --dataset CIFAR-10 \
    --model vit-s \
    --softmax base2 \
    --lr 1e-3 \
    --scheduler step \
    --warmup_epochs 5
```

---

## ğŸ“ˆ é¢„æœŸç»“æœ

æ ¹æ®åˆæ­¥å‡è®¾ï¼š

### å‡è®¾1: è®­ç»ƒç¨³å®šæ€§
- **é¢„æœŸ**: Base-2 Softmaxäº§ç”Ÿæ›´å¹³æ»‘çš„æ¢¯åº¦
- **éªŒè¯**: æŸ¥çœ‹ `gradient_norms.png` å›¾è¡¨
- **æŒ‡æ ‡**: æ¢¯åº¦L2èŒƒæ•°æ–¹å·®æ›´å°

### å‡è®¾2: æ¨¡å‹æ€§èƒ½
- **é¢„æœŸ**: ä¸¤ç§Softmaxæ€§èƒ½ç›¸å½“ï¼Œæˆ–Base-2åœ¨æŸäº›æƒ…å†µä¸‹å› ç¨³å®šæ€§æ›´å¥½è€Œç•¥ä¼˜
- **éªŒè¯**: æŸ¥çœ‹ `final_comparison.png` æŸ±çŠ¶å›¾
- **æŒ‡æ ‡**: Top-1å‡†ç¡®ç‡å·®å¼‚ < 1%

### å‡è®¾3: æ”¶æ•›é€Ÿåº¦
- **é¢„æœŸ**: Base-2å¯èƒ½æ”¶æ•›ç¨æ…¢ä½†æ›´ç¨³å®š
- **éªŒè¯**: æŸ¥çœ‹ `accuracy_comparison.png` æ›²çº¿
- **æŒ‡æ ‡**: è¾¾åˆ°90%æœ€ä½³å‡†ç¡®ç‡æ‰€éœ€epochæ•°

---

## ğŸ› æ•…éšœæ’é™¤

### CUDAé”™è¯¯

```bash
# æ£€æŸ¥CUDAå¯ç”¨æ€§
python -c "import torch; print(torch.cuda.is_available())"

# æŒ‡å®šå•ä¸ªGPU
CUDA_VISIBLE_DEVICES=0 python scripts/train_enhanced.py ...
```

### å†…å­˜ä¸è¶³

```bash
# å‡å°batch size
python scripts/train_enhanced.py --batch_size 64 ...

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
python scripts/train_enhanced.py --gradient_accumulation_steps 4 ...
```

### æ•°æ®é›†ä¸‹è½½å¤±è´¥

```bash
# CIFAR-10/100å·²åœ¨ /amax/storage/nfs/spco/data/
ls /amax/storage/nfs/spco/data/

# æ‰‹åŠ¨ä¸‹è½½ImageNetï¼ˆå¦‚éœ€è¦ï¼‰
# è§£å‹åˆ° /amax/storage/nfs/spco/data/imagenet/
```

---

## ğŸ“ å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬å®éªŒæ¡†æ¶ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{base2_softmax_bitnet_2025,
    title={Base-2 Softmax in Ultra-Low-Bit Quantization: An Empirical Study},
    author={Your Name},
    year={2025},
    url={https://github.com/yourusername/base-2-bitnet}
}
```

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License


